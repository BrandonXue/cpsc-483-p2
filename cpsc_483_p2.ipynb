{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 2 - Polynomial Models for Boston House Prices\n",
    "\n",
    "## Group members\n",
    "   Michael Peralta (mikeperalta@csu.fullerton.edu)\n",
    "   \n",
    "   Brandon Xue (brandonx@csu.fullerton.edu)\n",
    "\n",
    "### CPSC 483-02, Section ID: 33018"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 1\n",
    "Load and examine the Boston dataset’s features, target values, and description."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset the environment variables without confirmation.\n",
    "# This avoids broken code from functioning off of artifacts\n",
    "# left behind from previous executions.\n",
    "%reset -f\n",
    "\n",
    "# 1. Load and examine\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.datasets import *\n",
    "\n",
    "bs = load_boston()\n",
    "bs.DESCR # description\n",
    "bs.data # features\n",
    "bs.target # targets\n",
    "print(\"Features: \", end=\"\")\n",
    "print(*bs.feature_names, sep=\", \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 2\n",
    "Use *sklearn.model_selection.train_test_split()* to split the features and values into separate training and test sets. Use 80% of the original data as a training set, and 20% for testing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Partition into training and testing sets\n",
    "from IPython.display import display\n",
    "\n",
    "from sklearn import model_selection\n",
    "\n",
    "feature_frame = pd.DataFrame(data=bs.data, columns=bs.feature_names)\n",
    "\n",
    "# Add targets to same data frame so they stay paired with other data during the partition\n",
    "feature_frame.insert(len(feature_frame.columns), 'TARGET', bs.target)\n",
    "\n",
    "# train_test_split returns a list of the train-test split of inputs\n",
    "train_features, test_features = model_selection.train_test_split(feature_frame,\n",
    "                                               train_size=0.8,\n",
    "                                               random_state=113) # reproducible outputs\n",
    "\n",
    "# Using IPython.display allows us to view multiple data frames in the same execution\n",
    "# and use the pretty IPython format instead of the plain text of print()\n",
    "print(\"Training set:\")\n",
    "display(train_features)\n",
    "print(\"\\n\\nTest set:\")\n",
    "display(test_features)\n",
    "\n",
    "# Now that the set is partitioned, grab the targets into a separate data frame\n",
    "train_target = pd.DataFrame(data=train_features.iloc[:, -1:])\n",
    "test_target = pd.DataFrame(data=test_features.iloc[:, -1:])\n",
    "\n",
    "# And then remove the targets from the feature matrix\n",
    "train_features = train_features.drop(columns='TARGET')\n",
    "test_features = test_features.drop(columns='TARGET')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 3\n",
    "Create a *scatterplot* of the training set showing the relationship between the feature LSTAT and the target value MEDV.\n",
    "\n",
    "***Question***: Does the relationship appear to be linear?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Scatter plot of training data\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig_1, ax_1 = plt.subplots() # Obtain a tuple of a figure with one subplot\n",
    "\n",
    "ax_1.plot(train_features['LSTAT'], train_target, 'c.') # Cyan dots look nice\n",
    "\n",
    "# Use ax.set to set axis labels and a title\n",
    "retval = ax_1.set(xlabel=\"Training LSTAT (%)\", ylabel=\"Training MEDV (1000's USD)\",\n",
    "                 title=\"Training MEDV vs. Training LSTAT\")\n",
    "# Capture the return value in variable retval so it is not printed by the notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Does the relationship appear to be linear?\n",
    "\n",
    "The above relationship between the feature LSTAT and the target value MEDV does not quite appear to be linear. If we only examined LSTAT ranges from 10 to 25, it might, but the overall data appears to show a non-linear relationship."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 4\n",
    "With LSTAT as ***X*** and MEDV as ***t***, use *np.linalg.inv()* to compute ***w*** for the training set.\n",
    "\n",
    "***Question***: What is the equation for MEDV as a linear function of LSTAT?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the following formula to compute the weights:\n",
    "\n",
    "$$\n",
    "w = (X^TX)^{-1}X^Tt\n",
    "$$\n",
    "\n",
    "(found in the linear_regression_vectors_and_matrices.ipynb notebook)\n",
    "\n",
    "The equation for MEDV as a linear function of LSTAT is written after the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. A linear model for MEDV as a function of LSTAT\n",
    "\n",
    "# Copying Mike's idea, it makes sense to define a function to create the X matrix\n",
    "def make_poly_X(features, *, max_degree: int = 1, has_ones: bool = False):\n",
    "    \"\"\"Creates a copy of the inputted DataFrame, adding a column of 1.0\n",
    "       values to the left (if necessary). For each column apart from the\n",
    "       leftmost, higher ordered columns will be inserted just to the\n",
    "       right of their lower order counterparts.\n",
    "       \n",
    "        Intended Use:\n",
    "       \n",
    "            Provide features as a Pandas DataFrame. The columns must be\n",
    "            named and unique for this function to work. The features \n",
    "            should be in columns, i.e. each sample is a row.\n",
    "           \n",
    "        Parameters:\n",
    "        \n",
    "            features:    A DataFrame containing the features.\n",
    "            \n",
    "            max_degree:    Maximum degree/exponent that will be generated\n",
    "            for each column.\n",
    "            \n",
    "            has_ones:    Indicate whether the features DataFrame already\n",
    "            has a leftmost column of 1.0 values.\n",
    "    \"\"\"\n",
    "    \n",
    "    # If max_degree < 1, return immediately\n",
    "    if max_degree < 1:\n",
    "        return None\n",
    "    \n",
    "    # Handle Pandas Series objects by creating a frame out of them\n",
    "    if isinstance(features, pd.Series):\n",
    "        features = pd.DataFrame(data=features, columns=[features.name])\n",
    "    # If not a DataFrame at this point, there is an error\n",
    "    if not isinstance(features, pd.DataFrame):\n",
    "        return None\n",
    "    \n",
    "    # Make a copy to not affect original data\n",
    "    features = pd.DataFrame(features[features.columns])\n",
    "        \n",
    "    # Insert column of ones on the left if not already exists\n",
    "    if not has_ones:\n",
    "        features.insert(0, # insert at leftmost column\n",
    "                       'YINT_CONST', # name of column represents y-intercept constant\n",
    "                       np.ones_like(features.iloc[:,0]))\n",
    "        \n",
    "    # Grab all column names, ignoring the column of ones. This should have names of all features\n",
    "    feature_columns = features.columns.values[1:]\n",
    "    # Iterate over each feature\n",
    "    for feature_name in feature_columns:\n",
    "        feature_index = features.columns.get_loc(feature_name)\n",
    "        # For each feature, insert its higher degree columns sequentially\n",
    "        for exponent in range(2, max_degree+1):\n",
    "            features.insert(feature_index + exponent - 1, # column index to insert at\n",
    "                            feature_name + \"^\" + str(exponent), # column name\n",
    "                            features[feature_name].pow(exponent)) # Series multiplied\n",
    "            \n",
    "    # Return updated features\n",
    "    return features\n",
    "\n",
    "# Using our function, create our X matrix for a linear model\n",
    "lstat_vals = train_features['LSTAT']\n",
    "lstat_lin = make_poly_X(lstat_vals, max_degree=1, has_ones=False)\n",
    "display(lstat_lin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Continuing experiment 4, we will define a function to help us with repeated\n",
    "# calculations of weights in the later questions.\n",
    "\n",
    "def calc_weights(X_matrix: pd.DataFrame, targets: pd.DataFrame, method=\"inv\", verbose=False):\n",
    "    \"\"\"Calculates the weights of a model using the given X matrix and targets.\n",
    "    \n",
    "    Intended Use:\n",
    "    \n",
    "        Use make_poly_X to get X_matrix as a DataFrame.\n",
    "        Provide targets as a single column DataFrame.\n",
    "        \n",
    "    Parameters:\n",
    "    \n",
    "        method:    The method argument determines how this function will\n",
    "        calculate the weights. Using \"inv\" will invoke numpy.linalg.inv().\n",
    "        Using \"solve\" will invoke numpy.linalg.solve(), and in this case\n",
    "        the X_matrix must be of full rank. Using \"lstsq\" will invoke\n",
    "        numpy.linalg.lstsq().\n",
    "        \n",
    "        verbose:    When True, the matrices that are generated during the\n",
    "        computation of the weights will be printed, and the final weights\n",
    "        output will be displayed using IPython.display().\n",
    "    \"\"\"\n",
    "    \n",
    "    # Using linalg.lstsq, for situations where the matrix is not full rank.\n",
    "    if method == \"lstsq\":\n",
    "        solution, residuals, rank, singular_a = np.linalg.lstsq(X_matrix, targets, rcond=None)\n",
    "        weights = solution\n",
    "        if verbose:\n",
    "            print(\"lstsq residuals:\", residuals, sep=\"\\n\")\n",
    "            print(\"\\nlstsq rank:\", rank, sep=\"\\n\")\n",
    "            print(\"\\nsingular values:\", singular_a, sep=\"\\n\")\n",
    "        \n",
    "    # The other two methods both use XT_X and XT_t\n",
    "    else:\n",
    "        XT_X = np.dot(X_matrix.T, X_matrix) # X.transpose() dot X\n",
    "        XT_t = np.dot(X_matrix.T, targets) # X.transpose() dot target vector\n",
    "        if verbose:\n",
    "            print(\"XT_X:\", XT_X, sep=\"\\n\")\n",
    "            print(\"\\nXT_t:\", XT_t, sep=\"\\n\")\n",
    "        \n",
    "    # Using inverse, according to fcml Chapter 1 appendix, this is not ideal\n",
    "    # It can become slow and inaccurate\n",
    "    if method == \"inv\":\n",
    "        XT_X_inv = np.linalg.inv(XT_X)\n",
    "        if verbose:\n",
    "            print(\"\\nXT_X_inv:\", XT_X_inv, sep=\"\\n\")\n",
    "        weights = np.dot(XT_X_inv, XT_t)\n",
    "    \n",
    "    # Using linalg.solve, according to fcml Chapter 1 appexndix, is\n",
    "    # preferable compared to inverse\n",
    "    elif method == \"solve\":\n",
    "        weights = np.linalg.solve(XT_X, XT_t)\n",
    "        \n",
    "    # Create a list to rename the rows into w0, w1, ...\n",
    "    indices = [\"w\" + str(i) for i in range(len(X_matrix.columns))]\n",
    "    weights = pd.DataFrame(weights, # put weights into a data frame\n",
    "                           columns=['WEIGHTS'], # rename the column\n",
    "                           index=indices)  # rename the rows\n",
    "    \n",
    "    if verbose:\n",
    "        display(weights)\n",
    "    return pd.DataFrame(weights)\n",
    "    \n",
    "lstat_lin_weights = calc_weights(lstat_lin, train_target, method=\"inv\", verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Equation for MEDV as a linear function of LSTAT\n",
    "Using our training set, the equation for MEDV as a linear function of LSTAT is:\n",
    "\n",
    "\\begin{align}\n",
    "MEDV = 34.38436369 - (0.93878862 * LSTAT)\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 5\n",
    "\n",
    "Use **w** to add a line to your scatter plot from experiment *(3)*.\n",
    "\n",
    "***Question***: How well does the model appear to fit the training set?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Plotting and evaluating the linear MEDV v LSTAT model\n",
    "\n",
    "# Helper function for creating points for best fit line\n",
    "def make_x_y(X_column: pd.Series, weights: pd.DataFrame, degree=1):\n",
    "    \"\"\"Create a tuple (ndarray, ndarray) where the first ndarray\n",
    "       contains x-values, and the second ndarray is the corresponding\n",
    "       y-values. The output is directly compatible with matplotlib's\n",
    "       plotting functions.\n",
    "    \n",
    "    Intended Use:\n",
    "    \n",
    "        Provide X_column as a Pandas Series. Use the [] operator\n",
    "        on a data frame to get a Series. Use calc_weights to get\n",
    "        weights as a DataFrame. Set degree to match weights.\n",
    "        \n",
    "    Parameters:\n",
    "        \n",
    "        X_column:    A Pandas Series. Use the DataFrame[] operator\n",
    "        to get a column from the DataFrame as a Series object.\n",
    "        \n",
    "        weights:    A Pandas DataFrame. Use calc_weights() to get\n",
    "        weights as a DataFrame. Note the degree of the model used.\n",
    "        \n",
    "        degree:    The degree of the model. This must match the degree\n",
    "        used during calc_weights.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Generate uniform values across the range of [min(X_column), max(X_column)]\n",
    "    x_uniform = np.linspace(X_column.min(), X_column.max(), X_column.count())\n",
    "\n",
    "    # Put the new x values in a data frame to make the X matrix\n",
    "    X_matrix = pd.DataFrame(data=x_uniform, columns=[X_column.name])\n",
    "    X_matrix = make_poly_X(X_matrix, max_degree=degree, has_ones=False)\n",
    "    y_predictions = np.dot(X_matrix, weights)\n",
    "    return (x_uniform, y_predictions)\n",
    "\n",
    "\n",
    "lstat_linear_x, lstat_linear_y = make_x_y(lstat_vals, lstat_lin_weights, degree=1)\n",
    "\n",
    "fig_2, ax_2 = plt.subplots()\n",
    "# Scatter actual training data\n",
    "ax_2.plot(train_features['LSTAT'], train_target, 'c.')\n",
    "# Create line of best fit using points that are uniformly spaced in the x-axis\n",
    "ax_2.plot(lstat_linear_x, lstat_linear_y, 'r-', label=\"Linear Least Squares\")\n",
    "ax_2.legend()\n",
    "retval = ax_2.set(xlabel=\"Training LSTAT (%)\", ylabel=\"Training MEDV (1000's USD)\",\n",
    "                 title=\"Training MEDV vs. Training LSTAT\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How well does the model appear to fit the training set?\n",
    "5. The linear model of MEDV as a function of LSTAT does not appear to be a good fit for the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 6\n",
    "Use **w** to find the response for each value of the LSTAT attribute in the test set, then compute the test *MSE L* for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Computing MSE for the linear MEDV v LSTAT model on the test set\n",
    "\n",
    "# Create helper function to calculate MSE\n",
    "def calc_MSE(X_matrix: pd.DataFrame, weights: pd.DataFrame, targets: pd.DataFrame, verbose=False):\n",
    "    \"\"\"Calculate the mean squared error using the given X_matrix, weights,\n",
    "       and targets.\n",
    "       \n",
    "    Intended Use:\n",
    "    \n",
    "        Use make_poly_X to get X_matrix as a DataFrame.\n",
    "        Use calc_weights to get weights as a DataFrame.\n",
    "        Provide targets as a DataFrame.\n",
    "        \n",
    "    Parameters:\n",
    "    \n",
    "        X_matrix:    DataFrame containing the features with a column of\n",
    "        ones and higher ordered columns if applicable. This X_matrix\n",
    "        must have the same max_degree as what was used to calculate the\n",
    "        weights.\n",
    "        \n",
    "        weights:    DataFrame of weights (one column) that will be used\n",
    "        on the X_matrix to get predictions.\n",
    "        \n",
    "        targets:    Target values to compare predictions with.\n",
    "        \n",
    "        verbose:    If True, displays the residuals as a DataFrame.\n",
    "    \"\"\"\n",
    "    # This becomes a numpy ndarray\n",
    "    predictions = np.dot(X_matrix, weights) # X * w = y(hat)\n",
    "    \n",
    "    # This becomes a DataFrame again\n",
    "    residuals = targets - predictions # Residuals vector (observed - predicted)\n",
    "    residuals.columns = ['RESID']\n",
    "    if verbose:\n",
    "        display(residuals)\n",
    "    \n",
    "    # This becomes a numpy ndarray once more, with shape (1, 1)\n",
    "    sum_squared_error = np.dot(residuals.T, residuals) # dotting resids with self\n",
    "    # We need to grab the scalar value inside\n",
    "    sum_squared_error = sum_squared_error[0][0]\n",
    "    \n",
    "    # Now divide sum sq. err. w/ the number of values to get mean sq. err. (MSE)\n",
    "    return sum_squared_error / len(predictions)\n",
    "    \n",
    "# Grab the LSTAT values for the test set\n",
    "lstat_test_vals = test_features['LSTAT']\n",
    "\n",
    "# Create the X matrix for the test LSTAT\n",
    "lstat_test_lin = make_poly_X(lstat_test_vals, max_degree=1, has_ones=False)\n",
    "\n",
    "lstat_lin_MSE = calc_MSE(lstat_test_lin, lstat_lin_weights, test_target)\n",
    "print(\"Testing MSE for LSTAT linear model:\", lstat_lin_MSE)\n",
    "\n",
    "# Graphing the result of prediction on test set (optional)\n",
    "# lstat_lin_test_x, lstat_lin_test_y = make_x_y(lstat_test_vals, lstat_lin_weights, degree=1)\n",
    "# fig_3, ax_3 = plt.subplots()\n",
    "# # Scatter actual testing data\n",
    "# ax_3.plot(lstat_test_vals, test_target, 'c.')\n",
    "# # Create line of best fit using points that are uniformly spaced in the x-axis\n",
    "# ax_3.plot(lstat_lin_test_x, lstat_lin_test_y, 'r-', label=\"Prediction using weights from training\")\n",
    "# ax_3.legend()\n",
    "# retval = ax_3.set(xlabel=\"Test LSTAT (%)\", ylabel=\"Test MEDV (1000's USD)\",\n",
    "#                  title=\"Testing MEDV vs. Testing LSTAT\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  MSE For Linear Model\n",
    "\n",
    "6. The testing MSE for the LSTAT linear model was 39.95751318608688. The predictions are made using the weights derived from the training set, and are compared with the actual test data targets. The linear model still appears to be a poor predictor for MEDV."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 7\n",
    "Now add an *x 2* column to LSTAT’s *x* column in the training set, then repeat experiments *(4)* , *(5)*, and *(6)* for MEDV as a quadratic function of LSTAT.\n",
    "\n",
    "***Question***: Does the quadratic polynomial do a better job of predicting the values in the test set?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performing Experiment 4 on the New Quadratic Matrix\n",
    "With LSTAT as ***X*** and MEDV as ***t***, use *np.linalg.inv()* to compute ***w*** for the training set.\n",
    "\n",
    "***Question***: What is the equation for MEDV as a ***QUADRATIC*** function of LSTAT?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7 (4). Quadratic LSTAT model\n",
    "\n",
    "lstat_quad = make_poly_X(lstat_vals, max_degree=2, has_ones=False)\n",
    "display(lstat_quad)\n",
    "\n",
    "# We use the previously defined function, using the linalg.inv method\n",
    "lstat_quad_weights = calc_weights(lstat_quad, train_target, method=\"inv\")\n",
    "display(lstat_quad_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is the equation for MEDV as a ***QUADRATIC*** function of LSTAT?\n",
    "\n",
    "The equation for MEDV using the quadratic model is:\n",
    "\n",
    "\\begin{aligned}\n",
    "\\text{MEDV} = 41.8618826 - 2.163404190 * \\text{LSTAT} + 0.0382868299 * \\text{LSTAT}^2\n",
    "\\end{aligned}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performing Experiment 5 on the new Quadratic Matrix\n",
    "Use ***w*** to add a line to your scatter plot from experiment *(3)*.\n",
    "\n",
    "***Question***: How well does the model appear to fit the training set?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7 (5) Adding best fit line to scatter\n",
    "\n",
    "# Use our function to create points for the best-fit curve\n",
    "# Without this the points would be out of order and curve goes berserk\n",
    "lstat_quad_x, lstat_quad_y = make_x_y(lstat_vals, lstat_quad_weights, degree=2)\n",
    "\n",
    "# Plot best fit quad model on training data\n",
    "fig_4, ax_4 = plt.subplots()\n",
    "# Plot the actual training data\n",
    "ax_4.plot(lstat_vals, train_target, 'c.')\n",
    "# Draw the quadratic best fit curve\n",
    "ax_4.plot(lstat_quad_x, lstat_quad_y, 'r-', label=\"Quadratic model\\n(least squares)\")\n",
    "ax_4.legend()\n",
    "retval = ax_4.set(xlabel=\"Test LSTAT (%)\", ylabel=\"Test MEDV (1000's USD)\",\n",
    "                 title=\"Testing MEDV vs. Testing LSTAT\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How well does the model appear to fit the training set?\n",
    "7. (5) Visually, the model appears to fit the training data a lot better than the linear model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performing Experiment 6 on the New Quadratic Matrix\n",
    "Use ***w*** to find the response for each value of the LSTAT attribute in the test set, then\n",
    "compute the test *MSE L* for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7 (6) Computing MSE of the quadratic model\n",
    "\n",
    "# Recall that the LSTAT values for the test set are in l_stat_test_vals\n",
    "\n",
    "# Create the quad X matrix for the test LSTAT\n",
    "lstat_test_quad = make_poly_X(lstat_test_vals, max_degree=2, has_ones=False)\n",
    "\n",
    "lstat_quad_MSE = calc_MSE(lstat_test_quad, lstat_quad_weights, test_target, verbose=True)\n",
    "print(\"Testing MSE for LSTAT quadratic model:\", lstat_quad_MSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Does the quadratic polynomial do a better job of predicting the values in the test set?\n",
    "\n",
    "7. (6) The test MSE for the LSTAT quadratic model is lower than that of the linear model.\n",
    "\n",
    "Q: Does the quadratic polynomial do a better job of predicting the values in the test set?\n",
    "\n",
    "A: Yes, the quadratic polynomial has a lower testing MSE; therefore it predicts the values of the test set better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 8\n",
    "Repeat experiment *(4)* with all 13 input features as ***X*** and using *np.linalg.lstsq()*. (See the Appendix to *Linear regression in vector and matrix format* for details of why we need to switch away from *np.linalg.inv()*, and the notes for *np.linalg.solve()* for why we shouldn’t use that either.)\n",
    "\n",
    "***Question***: Does adding additional features improve the performance on the test set compared to using only LSTAT?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. All 13 features, linear model\n",
    "\n",
    "# Recall that the training features data frame is: train_features\n",
    "\n",
    "# Create X matrix for 13 features\n",
    "all_feat_lin = make_poly_X(train_features, max_degree=1, has_ones=False)\n",
    "\n",
    "# (XT X) w = (XT t)\n",
    "# Here, (XT X) is a square matrix, and (XT t) is a vector of dependent variables\n",
    "# Aw = b\n",
    "# np.linalg.solve(A, b) gives w\n",
    "\n",
    "# Later edit: the assignment prompt changed and we implemented calc_weights\n",
    "# to also allow method=\"lstsq\". This will use linalg.lstsq() in the calculation.\n",
    "\n",
    "afl_weights = calc_weights(all_feat_lin, train_target, method=\"lstsq\", verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. The equation for MEDV using all 13 features is:\n",
    "\n",
    "\\begin{aligned}\n",
    "\\text{MEDV}\\ =\\ & 33.5486261 \\\\\n",
    "& - 0.114331280(\\text{CRIM}) +  0.0330399664(\\text{ZN}) + 0.0219911151(\\text{INDUS}) \\\\\n",
    "& + 1.93047806(\\text{CHAS}) - 15.3459876(\\text{NOX}) +  4.11678898(\\text{RM}) \\\\\n",
    "& - 0.00520475977(\\text{AGE}) - 1.26111638(\\text{DIS}) + 0.352665352(\\text{RAD}) \\\\\n",
    "& - 0.0137375084(\\text{TAX}) - 1.01521476(\\text{PTRATIO}) + 0.00998692962(\\text{B}) \\\\\n",
    "& -0.490950094(\\text{LSTAT})\n",
    "\\end{aligned}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on test sest\n",
    "\n",
    "# Create X matrix with test features\n",
    "all_feat_lin_test = make_poly_X(test_features[test_features.columns.values], max_degree=1, has_ones=False)\n",
    "\n",
    "afl_MSE = calc_MSE(all_feat_lin_test, afl_weights, test_target, verbose=False)\n",
    "print(\"Testing MSE for 13 feature linear model:\", afl_MSE)\n",
    "print(\"Recall the testing MSE for LSTAT linear model:\", lstat_lin_MSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improvement with Additional Features?\n",
    "*Does adding additional features improve the performance on the test set compared to using only LSTAT?*\n",
    "\n",
    "8. Comparing the linear models, accuracy seems to improve with 13 features, as opposed to just LSTAT."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 9\n",
    "Now add $x^2$ columns for all 13 features, and repeat experiment *(8)*.\n",
    "\n",
    "***Question***: Does adding quadratic features improve the performance on the test set compared to using only linear features?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9. Quadratic model with all features\n",
    "\n",
    "# Recall that the training features data frame is: train_features\n",
    "\n",
    "# Create X matrix\n",
    "all_feat_quad = make_poly_X(train_features[train_features.columns.values], max_degree=2, has_ones=False)\n",
    "# Note that the CHAS (Charles River flag variable) is still the same after squaring,\n",
    "# because its values are either 0 or 1. This would make the matrix no longer linearly\n",
    "# independent, which has two problems:\n",
    "# 1. The dependent column adds no expressive value to our model. It is redundant.\n",
    "# 2. The dependence means there are infinite solutions and np.linalg.solve() won't work.\n",
    "#    Later edit: we are no longer using linalg.solve\n",
    "# Manually remove the CHAS^2 column to fix this.\n",
    "all_feat_quad = all_feat_quad.drop(columns='CHAS^2')\n",
    "\n",
    "afq_weights = calc_weights(all_feat_quad, train_target, method=\"lstsq\")\n",
    "display(afq_weights.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To compare performance, compute the test MSE for the all features quadratic model\n",
    "afq_test = make_poly_X(test_features[test_features.columns.values], max_degree=2, has_ones=False)\n",
    "\n",
    "# Once again make sure to drop CHAS^2\n",
    "afq_test = afq_test.drop(columns='CHAS^2')\n",
    "\n",
    "afq_MSE = calc_MSE(afq_test, afq_weights, test_target)\n",
    "print(\"Testing MSE for 13 feature quadratic model:\", afq_MSE)\n",
    "print(\"Recall the testing MSE for 13 feature linear model:\", afl_MSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improved Performance With Quadratic Features?\n",
    "Q: Does adding quadratic features improve the performance on the test set compared to using only linear features?\n",
    "\n",
    "A: Yes. It appears that the testing MSE has dropped significantly using the quadratic model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 10\n",
    "Compute the training MSE for experiments *(8)* and *(9)* and compare it to the test MSE.\n",
    "\n",
    "***Question***: What explains the difference?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10. Computing training MSE for both linear and quadratic 13-feature models.\n",
    "\n",
    "# Training MSE for 13 feature linear\n",
    "# Recall that the X matrix for the 13 feature linear model is: all_feat_lin\n",
    "# Recall that the weight vector for the 13 feature linear model is: afl_weights\n",
    "afl_train_MSE = calc_MSE(all_feat_lin, afl_weights, train_target)\n",
    "\n",
    "# Training MSE for 13 feature quadratic\n",
    "# Recall that the X matrix for the 13 feature quadratic model is: all_feat_quad\n",
    "# Recall that the weight vector for the 13 feature quadratic model is: afq_weights\n",
    "afq_train_MSE = calc_MSE(all_feat_quad, afq_weights, train_target)\n",
    "\n",
    "print(\"Training MSE for 13 feature linear:\", afl_train_MSE)\n",
    "print(\"Testing MSE for 13 feature linear:\", afl_MSE, end=\"\\n\\n\")\n",
    "\n",
    "print(\"Training MSE for 13 feature quadratic:\", afq_train_MSE)\n",
    "print(\"Testing MSE for 13 feature quadratic:\", afq_MSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What explains the difference?: Answer\n",
    "10. Although the testing MSE went up compared to the training MSE during the use of our 13-feature linear model, the linear model is the simplest model we can have, so we cannot be overfitting.\n",
    "\n",
    "  It appears our linear model is an underfit. Our quadratic model produced a much better training MSE, and the improvement persisted during our testing phase. This suggests that the true behavior is more closely modeled by the quadratic model than by the linear model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 11\n",
    "Repeat experiments *(9)* and *(10)*, adding *x^3* columns in addition to the existing *x* and *x^2* columns for each feature. Does the cubic polynomial do a better job of predicting the values in the training set?\n",
    "\n",
    "***Question***: Does it do a better job of predicting the values in the test set?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 11. Going to cubic\n",
    "\n",
    "# afc = all features cubic\n",
    "afc_train_X = make_poly_X(train_features[train_features.columns.values], max_degree=3, has_ones=False)\n",
    "afc_test_X = make_poly_X(test_features[test_features.columns.values], max_degree=3, has_ones=False)\n",
    "\n",
    "# As with the quadratic model, we have to remove higher order of the CHAS column\n",
    "# to avoid linear dependence.\n",
    "afc_train_X = afc_train_X.drop(columns=['CHAS^2', 'CHAS^3'])\n",
    "afc_test_X = afc_test_X.drop(columns=['CHAS^2', 'CHAS^3'])\n",
    "\n",
    "display(afc_train_X) # Take a look at this big frame\n",
    "\n",
    "# Calculate weights using training set\n",
    "afc_weights = calc_weights(afc_train_X, train_target, method=\"lstsq\")\n",
    "\n",
    "# Calculate MSE for training and test sets\n",
    "afc_train_MSE = calc_MSE(afc_train_X, afc_weights, train_target)\n",
    "afc_test_MSE = calc_MSE(afc_test_X, afc_weights, test_target)\n",
    "\n",
    "print(\"Training MSE for 13 feature cubic model:\", afc_train_MSE)\n",
    "print(\"Testing MSE for 13 feature cubic model:\", afc_test_MSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Improved Performance With Cubic Features?\n",
    "*Does adding **CUBIC** features improve the performance on the test set compared to using only **LINEAR OR QUADRATIC** features?* Does the cubic polynomial do a better job of predicting the values in the training set? Does it do a better job of predicting the values in the test set? Compute the MSE for experiments 8 and 9 and compare it to the test MSE. What explains the difference?*\n",
    "\n",
    "11. The cubic polynomial (13 feature model) predicts the training set better than its quadratic and linear counterparts. It also predicts the test set better than the quadratic and linear models. The training MSE for the cubic model was lower than the testing MSE. We can infer from this data that the cubic model is a better fit, and more closely models the true relationship between MEDV and the other 13 features."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}